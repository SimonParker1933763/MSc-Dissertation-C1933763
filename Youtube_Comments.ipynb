{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Access Youtube video comments\n",
        "\n",
        "\n",
        "##Import and General Variables"
      ],
      "metadata": {
        "id": "4-8vKm3AA2wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from googleapiclient.discovery import build\n",
        "#API verification\n",
        "api_key=\"AIzaSyDbARaxZ6Twi2H4aU-9NL8IfsARNwa7c_I\"#\"AIzaSyD89YoUZ3KxrS3sDxU-01qjWkk0UVvx0ZE\"\n",
        "#Youtube API library variable\n",
        "youtube = build('youtube','v3',developerKey=api_key)\n",
        "#Videos from channel CinemaStix from which comments were extracted\n",
        "video_ids='hq4Hw3Q2LBs', 'rkSgkxt7Ems', 'SXdYsoQcfj8', 'ItbCLh4Auoo', 'b3TT8Has8BE', 'Gw4155-zgDQ', 'WRr5uRmkNaQ', '_7PkAAkg_5Q', 'KawPEXiIW-I', 'DGQ9fQ8XV0o', 'CbnGb25daiw', 'oLuDf_PIWQ0'"
      ],
      "metadata": {
        "id": "gaYLNL2NGzxK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Comments"
      ],
      "metadata": {
        "id": "G_O17XVjt-Iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_comments(youtube):\n",
        "\n",
        "#Top 100 liked comments\n",
        "    #List to append to\n",
        "    responseTop=[]\n",
        "    for i in range(len(video_ids)):\n",
        "      requestTop = youtube.commentThreads().list(\n",
        "              part=\"snippet,replies\",\n",
        "              order=\"relevance\",\n",
        "              videoId=video_ids[i],\n",
        "              maxResults=1000)\n",
        "      Top= requestTop.execute()\n",
        "      responseTop.append(Top) #List of top comments for each video in JSON format\n",
        "\n",
        "#Unspecified order\n",
        "      #List to append to\n",
        "      responseAny=[]\n",
        "      requestAny = youtube.commentThreads().list(\n",
        "              part=\"snippet,replies\",\n",
        "              order=\"orderUnspecified\",\n",
        "              videoId=video_ids[i],\n",
        "              maxResults=10000)\n",
        "      Any= requestAny.execute()\n",
        "      responseAny.append(Any) #List of comments for each video in JSON format\n",
        "\n",
        "#Acquire statistics for top 100\n",
        "      #List to append to\n",
        "      BigTopData=[]\n",
        "      #Loop through every top comment and append list\n",
        "      for j in responseTop:\n",
        "        TopData=[]\n",
        "        for i in range (len(j['items'])):\n",
        "          TData= dict(Comment = j['items'][i]['snippet'][\"topLevelComment\"]['snippet']['textDisplay'],\n",
        "                    Like = j['items'][i]['snippet'][\"topLevelComment\"]['snippet']['likeCount'],\n",
        "                    Date = j['items'][i]['snippet'][\"topLevelComment\"]['snippet']['publishedAt'])\n",
        "          TopData.append(TData)\n",
        "        BigTopData.append(TopData) #List of top comments for each video in dict format\n",
        "\n",
        "\n",
        "#Acquire statistics for unspecified\n",
        "      #List to append to\n",
        "      BigAnyData=[]\n",
        "      for k in responseAny:\n",
        "        AnyData= []\n",
        "        for i in range (len(k['items'])):\n",
        "          AData= dict(Comment = k['items'][i]['snippet'][\"topLevelComment\"]['snippet']['textDisplay'],\n",
        "                    Like = k['items'][i]['snippet'][\"topLevelComment\"]['snippet']['likeCount'],\n",
        "                    Date = k['items'][i]['snippet'][\"topLevelComment\"]['snippet']['publishedAt'])\n",
        "          AnyData.append(AData)\n",
        "        BigAnyData.append(AnyData) #List of comments for each video in dict format\n",
        "\n",
        "    return BigTopData,BigAnyData\n",
        "\n",
        "#Variables for statistics on comments\n",
        "T100Data,Data= get_video_comments(youtube)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qBUrIX6Qq25l"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loop to index dicts and save data into CSV for statistics on comments from Cinemastix videos\n",
        "#For Top comments\n",
        "for i in range(len(T100Data)):\n",
        "  T100_Comments=pd.DataFrame(T100Data[i])\n",
        "  T100_Comments['Date']=pd.to_datetime(T100_Comments['Date']).dt.date\n",
        "  T100_Comments.to_csv('Cinemastix, T-C Video {}.csv'.format(i+1))\n",
        "#For Any comments\n",
        "for k in range(len(Data)):\n",
        "  Any_Comments=pd.DataFrame(Data[k])\n",
        "  Any_Comments['Date']=pd.to_datetime(Any_Comments['Date']).dt.date\n",
        "  Any_Comments.to_csv('CinemaStix, A-C Video {}.csv'.format(k+1))"
      ],
      "metadata": {
        "id": "HW_yDORWaIxO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pull All replies from Top comment"
      ],
      "metadata": {
        "id": "RTEPdSjsDA3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  #Function to pull replies from top comment\n",
        "  def commentreply(youtube):\n",
        "        request = youtube.comments().list(\n",
        "            part=\"snippet\",\n",
        "            maxResults=100,\n",
        "            #Top comment ID obtained from statistics result above\n",
        "            #Ideally would need to be able to index this out but I did not get round to doing it\n",
        "            parentId=\"UgzVQNl42i-EQmagB7t4AaABAg\")\n",
        "        response = request.execute() #List of replies in JSON format\n",
        "\n",
        "        #List to Append to\n",
        "        Replies=[]\n",
        "        #Loop every reply comment and retrieve statistics\n",
        "        for i in range(len(response['items'])):\n",
        "                R=dict(Reply=response['items'][i]['snippet']['textDisplay'],\n",
        "                       Likes=response['items'][i]['snippet']['likeCount'],\n",
        "                       Date=response['items'][i]['snippet']['publishedAt'])\n",
        "                Replies.append(R)\n",
        "\n",
        "        #Search through all other pages in the JSON file and retrieve statistics\n",
        "        next_page_token=response.get('nextPageToken')\n",
        "        more_pages=True\n",
        "\n",
        "        while more_pages:\n",
        "            if next_page_token is None:\n",
        "              more_pages = False\n",
        "            else:\n",
        "              request=youtube.comments().list(part=\"snippet\",\n",
        "                                              maxResults=50,\n",
        "                                              parentId=\"UgwyKmsiQ-pQFke1xuB4AaABAg\", #ALSO HERE\n",
        "                                              pageToken=next_page_token)\n",
        "              response=request.execute()\n",
        "\n",
        "              for i in range(len(response['items'])):\n",
        "                 R=dict(Reply=response['items'][i]['snippet']['textDisplay'],\n",
        "                        Likes=response['items'][i]['snippet']['likeCount'],\n",
        "                        Date=response['items'][i]['snippet']['publishedAt'])\n",
        "                 Replies.append(R)\n",
        "\n",
        "              next_page_token=response.get('nextPageToken')\n",
        "\n",
        "        return Replies\n",
        "\n",
        "#Save Data\n",
        "ReplyData=commentreply(youtube)\n",
        "RepliesTableR=pd.DataFrame(ReplyData)\n",
        "RepliesTableR['Date']=pd.to_datetime(RepliesTableR['Date']).dt.date\n",
        "RepliesTable=RepliesTableR[::-1].reset_index(drop=True)\n",
        "RepliesTable.to_csv('replies.csv')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "lURRpliB4Cxy"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}